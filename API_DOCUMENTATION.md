# API –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ê–ª–≥–æ—Ä–∏—Ç–º—ã –û–±—É—á–µ–Ω–∏—è

–≠—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ 5 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —Å PyTorch.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
import torch
import torch.nn as nn
from advanced_optimizers import (
    create_fer_optimizer,
    create_gradient_correction_optimizer, 
    create_sigprop_optimizer,
    create_localized_optimizer,
    ZeroShotTransferOptimizer
)

# –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –∏–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
optimizer = create_fer_optimizer(model.parameters(), lr=0.001)

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è PyTorch
for batch in dataloader:
    optimizer.zero_grad()
    loss = loss_fn(model(batch.x), batch.y)
    loss.backward()
    optimizer.step()
```

---

## üìö –ê–ª–≥–æ—Ä–∏—Ç–º—ã

### 1. üîÑ FER (Flipping Error Reduction)

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**: –£–º–µ–Ω—å—à–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –ø—É—Ç–µ–º –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

#### `create_fer_optimizer(params, base_optimizer_class=torch.optim.Adam, lr=0.001, fer_weight=0.1, **kwargs)`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `params` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- `base_optimizer_class` - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Adam)
- `lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `fer_weight` - –≤–µ—Å FER —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (0.01-0.5, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.1)
- `consistency_threshold` - –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.9)
- `memory_size` - —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ü–æ–≤—ã—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –£–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
- ‚úÖ –ù–∏–∑–∫–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã
- ‚ö†Ô∏è –ù–µ–±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–∏–º–µ—Ä:**
```python
optimizer = create_fer_optimizer(
    model.parameters(),
    base_optimizer_class=torch.optim.Adam,
    lr=0.001,
    fer_weight=0.1
)
```

---

### 2. üéØ Gradient Correction

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.

#### `create_gradient_correction_optimizer(params, lr=0.001, use_gcw=True, use_gcode=True, device='cpu')`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `params` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
- `lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `use_gcw` - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GC-W –º–æ–¥—É–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
- `use_gcode` - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GC-ODE –º–æ–¥—É–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
- `gcw_lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è GC-W (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01)
- `gcode_alpha` - –ø–∞—Ä–∞–º–µ—Ç—Ä –∞–ª—å—Ñ–∞ –¥–ª—è GC-ODE (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1)
- `device` - —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- ‚úÖ –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
- ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∏–º–µ—Ä:**
```python
optimizer = create_gradient_correction_optimizer(
    model.parameters(),
    lr=0.001,
    use_gcw=True,
    use_gcode=True,
    device=device
)
```

---

### 3. üß† Forward Signal Propagation

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**: –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä—è–º—ã–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–∏–≥–Ω–∞–ª–æ–≤.

#### `create_sigprop_optimizer(model, lr=0.01, signal_lr=0.001, **kwargs)`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `model` - –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (–ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å, –Ω–µ —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
- `lr` - –æ—Å–Ω–æ–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `signal_lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤
- `local_loss_weight` - –≤–µ—Å –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1)
- `signal_momentum` - momentum –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.9)

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
- ‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- ‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–ª—è –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä:**
```python
# –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å SigProp
model = SimpleSigPropNet(input_dim=784, hidden_dims=[512, 256], output_dim=10)
optimizer = create_sigprop_optimizer(
    model,
    lr=0.001,
    signal_lr=0.001
)

# –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è
for batch in dataloader:
    optimizer.zero_grad()
    outputs = model(batch.x)
    loss = loss_fn(outputs, batch.y)
    loss.backward()
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —à–∞–≥ –¥–ª—è ForwardSignal
    if isinstance(optimizer, ForwardSignalOptimizer):
        optimizer.step(model_output=outputs, loss=loss)
    else:
        optimizer.step()
```

---

### 4. üéØ Localized Learning

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**: –õ–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ –•–µ–±–±–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

#### `create_localized_optimizer(model, lr=0.01, hebbian_lr=0.001, **kwargs)`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `model` - –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (–ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å)
- `lr` - –æ—Å–Ω–æ–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `hebbian_lr` - —Å–∫–æ—Ä–æ—Å—Ç—å –¥–ª—è –•–µ–±–±–æ–≤—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- `localized_layers` - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–µ–≤ –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- `supervision_strength` - —Å–∏–ª–∞ —Å—É–ø–µ—Ä–≤–∏–∑–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1)

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ü—Ä–∞–≤–∏–ª–∞ –•–µ–±–±–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π

**–ü—Ä–∏–º–µ—Ä:**
```python
optimizer = create_localized_optimizer(
    model,
    lr=0.001,
    hebbian_lr=0.0001
)

# –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è
for batch in dataloader:
    optimizer.zero_grad()
    outputs = model(batch.x)
    loss = loss_fn(outputs, batch.y)
    loss.backward()
    
    # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ—Ç–µ—Ä–∏ –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    optimizer.step(loss=loss)
```

---

### 5. üöÄ Zero-Shot Transfer (ŒºTransfer)

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**: –ü–µ—Ä–µ–Ω–æ—Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è.

#### `ZeroShotTransferOptimizer(params, model, base_optimizer_kwargs={'lr': 0.001}, **kwargs)`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `params` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
- `model` - –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (–¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–æ–≤ —Å–ª–æ–µ–≤)
- `base_optimizer` - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é AdamW)
- `base_optimizer_kwargs` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–∑–æ–≤–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
- `mu_parametrization` - ŒºP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- `layer_mapping` - –º–∞–ø–ø–∏–Ω–≥ —Å–ª–æ–µ–≤ –Ω–∞ —Ç–∏–ø—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–µ–π –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ ŒºP –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ª–æ–µ–≤
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∏–º–µ—Ä:**
```python
optimizer = ZeroShotTransferOptimizer(
    model.parameters(),
    model,
    base_optimizer_kwargs={'lr': 0.001, 'weight_decay': 0.01}
)

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
for batch in dataloader:
    optimizer.zero_grad()
    loss = loss_fn(model(batch.x), batch.y)
    loss.backward()
    optimizer.step()
```

---

## üîß –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞

| –ó–∞–¥–∞—á–∞ | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º | –ü—Ä–∏—á–∏–Ω–∞ |
|--------|------------------------|---------|
| –û–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ | **FER** | –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö |
| –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | **Gradient Correction** | –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è |
| –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ | **Localized Learning** | –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ |
| –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π | **Zero-Shot Transfer** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ |
| –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è | **Forward Signal** | –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ |

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

1. **FER**: –ù–∞—á–Ω–∏—Ç–µ —Å `fer_weight=0.1`, —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –ø—Ä–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
2. **Gradient Correction**: –ù–∞—á–Ω–∏—Ç–µ —Å –º–∞–ª—ã—Ö `gcw_lr=0.001`
3. **Localized Learning**: `hebbian_lr` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ 10-100 —Ä–∞–∑ –º–µ–Ω—å—à–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ `lr`
4. **Forward Signal**: `signal_lr` –æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ `lr`
5. **Zero-Shot Transfer**: –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

### –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è FER
if hasattr(optimizer, 'get_fer_stats'):
    stats = optimizer.get_fer_stats()
    print(f"FER —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: {stats}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º PyTorch API
2. **–ü–∞–º—è—Ç—å**: Localized Learning –∏ Forward Signal —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏
3. **–°–∫–æ—Ä–æ—Å—Ç—å**: FER –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞
4. **Debugging**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `optimizer.state_dict()` –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è

---

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

–ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ MNIST:

| –ê–ª–≥–æ—Ä–∏—Ç–º | –¢–æ—á–Ω–æ—Å—Ç—å | –í—Ä–µ–º—è/—ç–ø–æ—Ö–∞ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|----------|----------|-------------|--------------|
| **Adam** (baseline) | 95.13% | 3.60s | –°—Ç–∞–Ω–¥–∞—Ä—Ç |
| **FER** | 73.00% | 6.49s | ‚≠ê –õ—É—á—à–∏–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π |
| **SGD** (baseline) | 85.05% | 3.58s | –ë–∞–∑–æ–≤—ã–π |
| **Localized Learning** | 84.67% | 3.53s | –ë—ã—Å—Ç—Ä—ã–π |
| **Gradient Correction** | 50.22% | 3.88s | –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ |

**–í—ã–≤–æ–¥**: FER –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ –≥–æ—Ç–æ–≤ –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é.